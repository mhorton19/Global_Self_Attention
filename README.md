# Global_Self_Attention
My experiments using global self-attention layer to augment/replace transformer architectures in NLP tasks
